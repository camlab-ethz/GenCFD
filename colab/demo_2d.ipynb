{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GenCFD 2D Demo\n",
    "\n",
    "This demo showcases how the GenCFD diffusion model can be used as a surrogate model to solve the 2D wave equation, utilizing a sine basis representation. The wave equation is a second-order linear partial differential equation (PDE) that describes the evolution of waves or standing wave fields. Examples include sound waves, seismic waves, or other wave-like phenomena arising in fields such as acoustics, electromagnetism, or fluid dynamics.\n",
    "\n",
    "In this setting, the wave equation is a hyperbolic PDE described as a scalar function in both space and time. For fluid dynamics, the quantity of interest is often represented as\n",
    "\n",
    "$$\n",
    "u = u(x,y,t).\n",
    "$$\n",
    "\n",
    "Here u(x,y,t) represents physical quantities in scalar fields, such as pressure or displacement. The governing equation for these fields is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 u}{\\partial t^2} = c^2 \\left( \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} \\right) ,\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- c: A fixed, non-negative real coefficient representing the wave propagation speed,\n",
    "- u: The scalar quantity of interest,\n",
    "- x,y: Spatial coordinates,\n",
    "- t: Time.\n",
    "\n",
    "This problem is fundamental in physics and engineering and serves as a building block for understanding more complex wave dynamics. In this demo, the GenCFD diffusion model acts as a surrogate to approximate the solution efficiently by leveraging the sine basis representation.\n",
    "\n",
    "For more information, visit the [Wave Equation](https://en.wikipedia.org/wiki/Wave_equation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install GenCFD as a Library\n",
    "\n",
    "To use GenCFD as a library, you can install it directly from its GitHub repository. Before proceeding, ensure that you are working within a conda or virtual environment to manage dependencies effectively and avoid conflicts. Once the environment is set up, run the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/camlab-ethz/GenCFD.git@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup relevant libraries\n",
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Tuple, Union\n",
    "\n",
    "from GenCFD import diffusion as dfn_lib\n",
    "from GenCFD import model, train, solvers, utils\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "array = np.ndarray\n",
    "\n",
    "DATA_STD = 0.5 # Fixed parameter but can also be learnable\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 0\n",
    "\n",
    "# Setting global seed for reproducibility\n",
    "torch.manual_seed(SEED)  # For CPU operations\n",
    "torch.cuda.manual_seed(SEED)  # For GPU operations\n",
    "torch.cuda.manual_seed_all(SEED)  # Ensure all GPUs (if multi-GPU) are set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Wave Equation\n",
    "\n",
    "In this demo, we solve the 2D wave equation using a sine basis representation. Below we detail the generation of the sine basis, the matrix describing the wave modes, and the analytical solution of the wave equation.\n",
    "\n",
    "1. Sine Basis Generation\n",
    "\n",
    "The sine basis is used to represent the solution as a sum of sinusoidal components. Each basis function corresponds to a specific mode (i,j), representing:\n",
    "\n",
    "$$\n",
    "sin(\\pi i x) sin(\\pi j y) .\n",
    "$$\n",
    "\n",
    "These functions are evaluated on a uniform grid of size s x s. The following function generates the sine basis of shape (K, K, s, s) where K is the number of modes in each spatial dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Sine basis B of the shape (K, K, s, s)\n",
    "# For each mode (i,j), B[i,j] represents the function \n",
    "# sin(np.pi*i*x)*np.sin(np.pi*j*y) evaluated at the s*s uniform grid\n",
    "def generate_sine_basis(K = 16, s = 128):\n",
    "    xx,yy = np.meshgrid(np.arange(s), np.arange(s), indexing=\"ij\")\n",
    "    xx = xx/s\n",
    "    yy = yy/s\n",
    "    sine_basis  = np.zeros((K,K,s,s))\n",
    "    for i in range(1,K+1):\n",
    "        for j in range(1,K+1):  \n",
    "            sine_basis[i-1,j-1] = np.sin(np.pi*i*xx)*np.sin(np.pi*j*yy)\n",
    "    return sine_basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Wave Mode Matrix\n",
    "\n",
    "To capture the frequency and decay properties of the wave equation, a square matrix is generated where each element (i,j) corresponds to:\n",
    "\n",
    "$$\n",
    "(i+1)^2 + (j+1)^2\n",
    "$$\n",
    "\n",
    "This matrix is essential for determining the mode-dependent contributions to the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_matrix(K=16):\n",
    "    M = np.zeros((K, K))\n",
    "    for i in range(1, K + 1):\n",
    "        for j in range(1, K + 1):\n",
    "            M[i - 1, j - 1] = i**2 + j**2\n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Analytical Solution of the 2D Wave Equation\n",
    "\n",
    "The analytical solution is computed using the sine basis and wave mode matrix. The scalar field u(x,y,t) is represented as a summation of sinusoidal components, modulated by coefficients, decay terms, and time-dependent oscillations:\n",
    "\n",
    "- f(x,y): Initial condition of the wave equation.\n",
    "- u(x,y,t): Solution at a given time t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytical solution of 2d Wave equation\n",
    "def generate_solution_wave(coeff, time, sine_basis, square_matrix, K = 16, decay = 0.8, c = 0.1):\n",
    "    multiplier_f = coeff * np.power(square_matrix, decay)\n",
    "    multiplier_f = multiplier_f.reshape(K, K , 1, 1)\n",
    "    f = np.pi*np.sum(multiplier_f * sine_basis, axis = (0,1))\n",
    "\n",
    "    square_matrix_time = np.cos(c * np.pi * time * np.sqrt(square_matrix))\n",
    "    multiplier_u = coeff * np.power(square_matrix, decay) * square_matrix_time\n",
    "    multiplier_u = multiplier_u.reshape(K, K , 1, 1)\n",
    "    u = np.pi*np.sum(multiplier_u * sine_basis, axis = (0,1))\n",
    "\n",
    "    return f, u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Micro-Macro Perturbations\n",
    "\n",
    "GenCFD has demonstrated the ability to capture not only statistical patterns but also make accurate predictions for out-of-distribution scenarios. To evaluate this capability, GenCFD is tested on a dataset generated with small perturbations in the initial conditions - perturbations it has never encountered during training.\n",
    "\n",
    "these minor variations in the initial conditions can lead to significant changes in the target solution, a characteristic commonly observed in turbulent flows and chaotic systems. Consequently, this dataset is exclusively used for inference and validation, highlighting GenCFD's robustness in handling unpredictable and dynamic scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of micro-macro perturbations\n",
    "def generate_perturbation_wave(coeff, time, sine_basis, square_matrix, K = 16, decay = 0.8, c =0.1, perturbation = 0.25):\n",
    "    p_matrix = np.random.uniform(-perturbation, perturbation, (K, K))\n",
    "    return generate_solution_wave(coeff+p_matrix , time, sine_basis, square_matrix, K = K, decay = decay, c = c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation for Training and Evaluation\n",
    "\n",
    "The generate_training_data function creates a dataset of input-output pairs for training the model, while the generate_micro_macro_data function generates datasets with micro and macro perturbations for evaluating the model's robustness to chaotic and turbulent scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(c = 0.1, time = 5.0, N_data = 1024, K = 24, decay = -0.8, s = 128):\n",
    "    sine_basis = generate_sine_basis(K, s)\n",
    "    square_matrix = generate_square_matrix(K)\n",
    "\n",
    "    inputs = np.zeros((N_data, s, s))\n",
    "    targets = np.zeros((N_data, s, s))\n",
    "    \n",
    "    for n in range(N_data):\n",
    "        coeff = np.random.uniform(-1,1, (K, K))\n",
    "        inp, out = generate_solution_wave(coeff, time, sine_basis, square_matrix, K = K, decay = decay, c = c)\n",
    "        inputs[n] = inp\n",
    "        targets[n] = out\n",
    "\n",
    "        if n%20 == 0:\n",
    "            print(f\"Done {n+1} out of {N_data}\")\n",
    "        \n",
    "    print(\" \")\n",
    "\n",
    "    return inputs, targets\n",
    "\n",
    "def generate_micro_macro_data(coeff_macro, perturbation = 0.25, c = 0.1, time = 5.0, N_data = 1024, K = 24, decay = -0.8, s = 128):\n",
    "    sine_basis = generate_sine_basis(K, s)\n",
    "    square_matrix = generate_square_matrix(K)\n",
    "\n",
    "    inputs = np.zeros((N_data, s, s))\n",
    "    targets = np.zeros((N_data, s, s))\n",
    "    \n",
    "    for n in range(N_data):\n",
    "        inp, out = generate_perturbation_wave(coeff_macro, time, sine_basis, square_matrix, K = K, decay = decay, c = c, perturbation = perturbation)\n",
    "        inputs[n] = inp\n",
    "        targets[n] = out\n",
    "\n",
    "        if n%20 == 0:\n",
    "            print(f\"Done {n+1} micro out of {N_data}\")\n",
    "\n",
    "    print(\" \")\n",
    "\n",
    "    return inputs, targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation of training and evaluation samples. Defining key parameters for the wave equation simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 1 out of 128\n",
      "Done 21 out of 128\n",
      "Done 41 out of 128\n",
      "Done 61 out of 128\n",
      "Done 81 out of 128\n",
      "Done 101 out of 128\n",
      "Done 121 out of 128\n",
      " \n",
      "Done 1 micro out of 128\n",
      "Done 21 micro out of 128\n",
      "Done 41 micro out of 128\n",
      "Done 61 micro out of 128\n",
      "Done 81 micro out of 128\n",
      "Done 101 micro out of 128\n",
      "Done 121 micro out of 128\n",
      " \n"
     ]
    }
   ],
   "source": [
    "K = 24\n",
    "s = 128\n",
    "decay = -0.8\n",
    "c = 0.1\n",
    "time = 5.0\n",
    "N_data = 128\n",
    "\n",
    "inp_train, out_train = generate_training_data(c = c, time = time, N_data = N_data, K = K, decay =decay, s = s)\n",
    "\n",
    "N_data_macro = 128\n",
    "perturbation = 0.25\n",
    "coeff_macro = np.random.uniform(-1,1, (K, K))\n",
    "\n",
    "inp_micro, out_micro = generate_micro_macro_data(coeff_macro, perturbation = perturbation, c = c, time = time, N_data = N_data_macro, K = K, decay = decay, s = s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visuzalization\n",
    "\n",
    "The Initial Condition represents the input to the model, while the Solution repersents the target output that the model aims to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].imshow(inp_train[i,...])\n",
    "axes[0].set_title(\"Initial Condition\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(out_train[i,...])\n",
    "axes[1].set_title(\"Solution\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BaseDataset class serves as a foundation for managing both training and evaluation datasets. It standardizes data input/output for consistent model training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset:\n",
    "    \"\"\"\n",
    "    Base class for Inference and Training. \n",
    "\n",
    "    Args:\n",
    "        input_data: Training Input Dataset, typically includes the initial conditions\n",
    "        output_data: Ground Truth (GT), Output Dataset, is the solution to the given problem\n",
    "        mode: Decision whether the given data is used for inference or training thus only 2 options ['training', 'evaluation']\n",
    "        input_channel: Number of input channels (from train_data)\n",
    "        output_channel: Number of channels that should be predicted by the Diffusion Model\n",
    "        spatial_resolution: Resolution of the input and output dataset\n",
    "        input_shape: Shape of the input data tensor. Depends on the conditioning you use\n",
    "        output_shape: This should be equal to (output_channel, spatial_resolution)\n",
    "        ndim: Dimensionality of the dataset. Here we have a 2D dataset\n",
    "        training_samples: Number of samples available in the dataset\n",
    "        mean_training_input: Array with relevant mean values for each channel of the input dataset\n",
    "        std_training_input: Array with relevant std values for each channel of the input dataset\n",
    "        mean_training_output: Array with relevant mean values for each channel of the output dataset\n",
    "        std_training_output: Array with relevant std values for each channel of the output dataset\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            input_data: Dataset,\n",
    "            output_data: Dataset,\n",
    "            mode: str,\n",
    "            input_channel: int,\n",
    "            output_channel: int,\n",
    "            spatial_resolution: Tuple,\n",
    "            input_shape: Tuple,\n",
    "            output_shape: Tuple,\n",
    "            ndim: int = 2,\n",
    "            training_samples: int = None,\n",
    "            mean_training_input: array = None,\n",
    "            std_training_input: array = None,\n",
    "            mean_training_output: array = None,\n",
    "            std_training_output: array = None\n",
    "        ) -> None:\n",
    "\n",
    "        assert mode in ['training', 'evaluation'], f'mode can either be training or evaluation not {mode}'\n",
    "\n",
    "        self.initial_cond = input_data\n",
    "        self.target_cond = output_data\n",
    "        self.mode = mode\n",
    "        self.input_channel = input_channel\n",
    "        self.output_channel = output_channel\n",
    "        self.spatial_resolution = spatial_resolution\n",
    "        self.input_shape = input_shape,\n",
    "        self.output_shape = output_shape,\n",
    "        self.ndim = ndim,\n",
    "        self.training_samples = training_samples\n",
    "        self.mean_training_input = mean_training_input\n",
    "        self.std_training_input = std_training_input\n",
    "        self.mean_training_output = mean_training_output\n",
    "        self.std_training_output = std_training_output\n",
    "\n",
    "\n",
    "    def normalize(self, u_: Union[array, Tensor], which: str) -> Union[array, Tensor]:\n",
    "        \"\"\"Standardization of data can be done for torch tensors or numpy arrays\n",
    "        \n",
    "        args:\n",
    "            u_:     Container which should be standardized by subtracting the mean and dividing through the std\n",
    "            which:   decision whether to use the input or output normalization parameters (mean and std) \n",
    "        \"\"\"\n",
    "\n",
    "        assert which in ['input', 'output']\n",
    "\n",
    "        if self.mean_training_input is not None:\n",
    "            mean_training_input = self.mean_training_input if which == 'input' else self.mean_training_output\n",
    "            std_training_input = self.std_training_input if which == 'input' else self.std_training_output\n",
    "            if isinstance(u_, Tensor):\n",
    "                mean_training_input = torch.as_tensor(\n",
    "                    mean_training_input, dtype=u_.dtype, device=u_.device\n",
    "                )\n",
    "                std_training_input = torch.as_tensor(\n",
    "                    std_training_input, dtype=u_.dtype, device=u_.device\n",
    "                )\n",
    "            return (u_ - mean_training_input) / (std_training_input + 1e-12)\n",
    "        else:\n",
    "            return u_\n",
    "    \n",
    "\n",
    "    def denormalize(self, u_: Union[array, Tensor], which: str) -> Union[array, Tensor]:\n",
    "        \"\"\"Reverse the Standardization done for either the input or output container.\"\"\"\n",
    "\n",
    "        assert which in ['input', 'output']\n",
    "\n",
    "        if self.mean_training_input is not None:\n",
    "            mean_training_input = self.mean_training_input if which == 'input' else self.mean_training_output\n",
    "            std_training_input = self.std_training_input if which == 'input' else self.std_training_output\n",
    "            if isinstance(u_, Tensor):\n",
    "                mean_training_input = torch.as_tensor(\n",
    "                    mean_training_input, dtype=u_.dtype, device=u_.device\n",
    "                )\n",
    "                std_training_input = torch.as_tensor(\n",
    "                    std_training_input, dtype=u_.dtype, device=u_.device\n",
    "                )\n",
    "            return u_ * (std_training_input + 1e-12) + mean_training_input\n",
    "        else:\n",
    "            return u_\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.training_samples\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index: int) -> dict:\n",
    "        \n",
    "        initial_cond = self.initial_cond[index, ...].reshape((*self.spatial_resolution, 1))\n",
    "        target_cond = self.target_cond[index, ...].reshape((*self.spatial_resolution, 1))\n",
    "\n",
    "        if self.mode == 'training':\n",
    "            initial_cond = self.normalize(initial_cond, 'input')\n",
    "            target_cond = self.normalize(target_cond, 'output')\n",
    "        \n",
    "        # The data should be in the form (channel, y, x)\n",
    "        initial_condition = (\n",
    "            torch.from_numpy(initial_cond)\n",
    "            .type(torch.float32)\n",
    "            .permute(2, 1, 0)\n",
    "        )\n",
    "\n",
    "        target_condition = (\n",
    "            torch.from_numpy(target_cond)\n",
    "            .type(torch.float32)\n",
    "            .permute(2, 1, 0)\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'initial_cond': initial_condition,\n",
    "            'target_cond': target_condition\n",
    "        }\n",
    "\n",
    "\n",
    "mean_training_input = inp_train.mean()\n",
    "std_training_input = inp_train.std()\n",
    "mean_training_output = out_train.mean()\n",
    "std_training_output = out_train.std()\n",
    "\n",
    "batch_size = 5\n",
    "in_channels = 1 # number of channels of the initial condition\n",
    "out_channels = 1 # number of channels to predict\n",
    "spatial_resolution = (s, s) # (128, 128)\n",
    "input_shape = (2, s, s) # due to conditioning of the diff model on intitial cond\n",
    "output_shape = (out_channels, s, s) \n",
    "\n",
    "# standardization parameters should be stored as buffers in the model\n",
    "# This is relevant for inference to guarantee the same standardization is performed\n",
    "buffer_dict = {\n",
    "    \"mean_training_input\": torch.tensor(\n",
    "        mean_training_input, \n",
    "        dtype=torch.float32, \n",
    "        device=device\n",
    "    ),\n",
    "    \"mean_training_output\": torch.tensor(\n",
    "        mean_training_output,\n",
    "        dtype=torch.float32,\n",
    "        device=device\n",
    "    ),\n",
    "    \"std_training_input\": torch.tensor(\n",
    "        std_training_input,\n",
    "        dtype=torch.float32,\n",
    "        device=device\n",
    "    ),\n",
    "    \"std_training_output\": torch.tensor(\n",
    "        std_training_output,\n",
    "        dtype=torch.float32,\n",
    "        device=device\n",
    "    ),\n",
    "}\n",
    "\n",
    "train_dataset = BaseDataset(\n",
    "    input_data=inp_train,\n",
    "    output_data=out_train,\n",
    "    mode='training',\n",
    "    input_channel=in_channels,\n",
    "    output_channel=out_channels,\n",
    "    spatial_resolution=spatial_resolution, # (128, 128)\n",
    "    input_shape=input_shape,\n",
    "    output_shape=output_shape,\n",
    "    ndim=2,\n",
    "    training_samples=N_data, # 128 samples\n",
    "    mean_training_input=mean_training_input,\n",
    "    std_training_input=std_training_input,\n",
    "    mean_training_output=mean_training_output,\n",
    "    std_training_output=std_training_output\n",
    ")\n",
    "\n",
    "evaluation_dataset = BaseDataset(\n",
    "    input_data=inp_micro,\n",
    "    output_data=out_micro,\n",
    "    mode='evaluation',\n",
    "    input_channel=in_channels,\n",
    "    output_channel=out_channels,\n",
    "    spatial_resolution=spatial_resolution, \n",
    "    input_shape=input_shape,\n",
    "    output_shape=output_shape,\n",
    "    ndim=2,\n",
    "    training_samples=N_data, \n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "evaluation_dataloader = DataLoader(\n",
    "    dataset=evaluation_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Construction\n",
    "\n",
    "Section describes the creation and configuration of the denoising model, which consists of a UNet-based Preconditioned Denoiser wrapped within a diffusion framework for generating samples. To enhance performance, the model can be compiled using torch.compile. This provides faster training and inference, particularly on GPUs.\n",
    "\n",
    "Note: Some GPUs may produce warnings during compilation. These warnings can safely be ignored. For the following GPUs no Warnings were observed:\n",
    "- NVIDIA GeForce RTX 2080 Ti\n",
    "- NVIDIA GeForce RTX 3090\n",
    "- NVIDIA GeForce RTX 4090\n",
    "- NVIDIA Tesla A100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of model parameters: 5599233\n"
     ]
    }
   ],
   "source": [
    "denoiser = model.PreconditionedDenoiser(\n",
    "    in_channels=in_channels + out_channels, # Conditioning thus stacked input and output\n",
    "    out_channels=out_channels,\n",
    "    spatial_resolution=spatial_resolution,\n",
    "    time_cond=False,\n",
    "    num_channels=(64, 128),\n",
    "    downsample_ratio=(2, 2),\n",
    "    num_blocks=4,\n",
    "    noise_embed_dim=128,\n",
    "    output_proj_channels=128,\n",
    "    input_proj_channels=128,\n",
    "    padding_method='circular',\n",
    "    dropout_rate=0.0,\n",
    "    use_attention=True,\n",
    "    use_position_encoding=True,\n",
    "    num_heads=8,\n",
    "    normalize_qk=False,\n",
    "    dtype=torch.float32,\n",
    "    device=device,\n",
    "    buffer_dict=buffer_dict,\n",
    "    sigma_data=DATA_STD\n",
    ")\n",
    "\n",
    "# Optional: compile the model for fast training and inference\n",
    "is_compiled = True\n",
    "if torch.cuda.is_available() and is_compiled:\n",
    "    denoiser = torch.compile(denoiser)\n",
    "    # For some GPUs there will appear Warnings, if that's the case \n",
    "    # these can be ignored.\n",
    "    warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "diffusion_scheme = dfn_lib.Diffusion.create_variance_exploding(\n",
    "    sigma=dfn_lib.tangent_noise_schedule(device=device),\n",
    "    data_std=DATA_STD,\n",
    ")\n",
    "\n",
    "denoising_model = dfn_lib.DenoisingModel(\n",
    "    spatial_resolution=spatial_resolution,\n",
    "    time_cond=False,\n",
    "    denoiser=denoiser,\n",
    "    noise_sampling=dfn_lib.log_uniform_sampling(\n",
    "        diffusion_scheme, \n",
    "        clip_min=1e-4, \n",
    "        uniform_grid=True, \n",
    "        device=device\n",
    "    ),\n",
    "    noise_weighting=dfn_lib.edm_weighting(\n",
    "        data_std=DATA_STD, \n",
    "        device=device\n",
    "    ),\n",
    "    consistent_weight=0.0,\n",
    "    device=device,\n",
    "    dtype=torch.float32,\n",
    "    task='solver'\n",
    ")\n",
    "\n",
    "# Print number of Parameters:\n",
    "model_params = sum(\n",
    "    p.numel() for p in denoising_model.denoiser.parameters() if p.requires_grad\n",
    ")\n",
    "\n",
    "print(f\"Total number of model parameters: {model_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DenoisingTrainer handles the training process for the denoising model. It includes features like Exponential Moving Average (EMA), which is crucial for stabilizing training and should always be enabled for consistent and reliable inference. Additionally, mixed precision training is utilized to optimize memory usage and computational efficiency by leveraging float16 or bfloat16 precision, making it ideal for modern GPUs and large-scale models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = train.DenoisingTrainer(\n",
    "    model=denoising_model,\n",
    "    optimizer=optim.AdamW(\n",
    "        denoising_model.denoiser.parameters(),\n",
    "        lr=1e-4,\n",
    "        weight_decay=0.01,\n",
    "    ),\n",
    "    device=device,\n",
    "    ema_decay=0.999,\n",
    "    store_ema=True,\n",
    "    track_memory=False,\n",
    "    use_mixed_precision=True,\n",
    "    is_compiled=is_compiled\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command launches the training, where metric_aggregation_steps specifies the interval, in terms of iteration steps, after which metrics like loss or the standard deviation of the loss should be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?step/s]W0124 18:16:27.760000 140507766322752 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] d0 is not in var_ranges, defaulting to unknown range.\n",
      "W0124 18:16:27.761000 140507766322752 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] d1 is not in var_ranges, defaulting to unknown range.\n",
      "W0124 18:16:27.761000 140507766322752 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] d2 is not in var_ranges, defaulting to unknown range.\n",
      "W0124 18:16:27.772000 140507766322752 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] d0 is not in var_ranges, defaulting to unknown range.\n",
      "W0124 18:16:27.773000 140507766322752 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] d2 is not in var_ranges, defaulting to unknown range.\n",
      "W0124 18:16:48.210000 140507766322752 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.\n",
      "W0124 18:16:48.215000 140507766322752 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.\n",
      "W0124 18:16:48.217000 140507766322752 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.\n",
      "W0124 18:16:48.234000 140507766322752 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.\n",
      "W0124 18:16:48.235000 140507766322752 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.\n",
      "W0124 18:16:48.236000 140507766322752 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.\n",
      "W0124 18:16:48.522000 140507766322752 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z1 is not in var_ranges, defaulting to unknown range.\n",
      "W0124 18:16:48.523000 140507766322752 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.\n",
      "W0124 18:16:48.524000 140507766322752 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z2 is not in var_ranges, defaulting to unknown range.\n",
      "W0124 18:17:08.498000 140507766322752 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] r2 is not in var_ranges, defaulting to unknown range.\n",
      "W0124 18:17:08.499000 140507766322752 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.\n",
      "W0124 18:17:08.499000 140507766322752 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.\n",
      "  6%|â–Œ         | 275/5000 [08:23<1:51:28,  1.42s/step, loss=tensor(1.0535, device='cuda:0')]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m num_train_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5_000\u001b[39m \u001b[38;5;66;03m# Can be also trained shorter e.g. 10_000 or even less steps\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetcwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# DDP configs (not used here thuse set to 1)\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Training configs\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_train_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_train_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Since the dataset is small, metric aggregation after every epoch\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_aggregation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Callbacks\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Save model at every checkpoint or at the end of the training\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrainStateCheckpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetcwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43msave_every_n_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_train_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# only save one model at the end\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Display training progress in a tqdm bar\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTqdmProgressBar\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtotal_train_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_train_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_monitors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Diffusion/venv/lib/python3.11/site-packages/GenCFD/train/training_loop.py:141\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(train_dataloader, trainer, workdir, world_size, local_rank, total_train_steps, metric_aggregation_steps, eval_dataloader, eval_every_steps, num_batches_per_eval, run_sanity_eval_batch, metric_writer, callbacks)\u001b[0m\n\u001b[1;32m    138\u001b[0m     step_diff_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m epochs_train_steps \u001b[38;5;241m-\u001b[39m cur_step\n\u001b[1;32m    139\u001b[0m     epochs_train_steps \u001b[38;5;241m=\u001b[39m epoch_train \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader) \u001b[38;5;241m-\u001b[39m step_diff_train\n\u001b[0;32m--> 141\u001b[0m train_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[1;32m    142\u001b[0m cur_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_steps\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_rank \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m metric_writer:\n",
      "File \u001b[0;32m~/Diffusion/venv/lib/python3.11/site-packages/GenCFD/train/trainers.py:91\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self, batch_iter, num_steps)\u001b[0m\n\u001b[1;32m     89\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(batch_iter)\n\u001b[1;32m     90\u001b[0m batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 91\u001b[0m metrics_update \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m train_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate(metrics_update[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     94\u001b[0m train_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_std\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate(metrics_update[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Diffusion/venv/lib/python3.11/site-packages/GenCFD/utils/train_utils.py:73\u001b[0m, in \u001b[0;36mcompute_memory.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_memory:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# Reset peak memory\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mreset_peak_memory_stats(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 73\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_memory:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# Compute the peak memory\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     peak_mem \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmax_memory_allocated(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m/\u001b[39m (\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     79\u001b[0m     )  \u001b[38;5;66;03m# Convert to GB\u001b[39;00m\n",
      "File \u001b[0;32m~/Diffusion/venv/lib/python3.11/site-packages/GenCFD/train/trainers.py:334\u001b[0m, in \u001b[0;36mDenoisingTrainer.train_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    332\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 334\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Diffusion/venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:454\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    452\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 454\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/Diffusion/venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:351\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/Diffusion/venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:351\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_train_steps = 5_000 # Can be also trained shorter e.g. 10_000 or even less steps\n",
    "\n",
    "train.run_training(\n",
    "    train_dataloader=train_dataloader,\n",
    "    trainer=trainer,\n",
    "    workdir=str(os.getcwd()),\n",
    "    # DDP configs (not used here thuse set to 1)\n",
    "    world_size=1,\n",
    "    local_rank=-1,\n",
    "    # Training configs\n",
    "    total_train_steps=num_train_steps,\n",
    "    # Since the dataset is small, metric aggregation after every epoch\n",
    "    metric_aggregation_steps=N_data // batch_size, \n",
    "    # Callbacks\n",
    "    callbacks=(\n",
    "        # Save model at every checkpoint or at the end of the training\n",
    "        train.callbacks.TrainStateCheckpoint(\n",
    "            base_dir=str(os.getcwd()),\n",
    "            save_every_n_step=num_train_steps # only save one model at the end\n",
    "        ),\n",
    "        # Display training progress in a tqdm bar\n",
    "        train.callbacks.TqdmProgressBar(\n",
    "            total_train_steps=num_train_steps,\n",
    "            train_monitors=[\"loss\",]\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "The model's latest checkpoint is restored from the specified directory. Relevant is to load the EMA model parameters for inference on the macro-micro perturbed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(str(os.getcwd()),'checkpoints')\n",
    "latest_model_path = utils.get_latest_checkpoint(model_path)\n",
    "\n",
    "trained_state = train.train_states.DenoisingModelTrainState.restore_from_checkpoint(\n",
    "    latest_model_path,\n",
    "    model=denoising_model.denoiser,\n",
    "    optimizer=trainer.optimizer,\n",
    "    is_compiled=trainer.is_compiled,\n",
    "    is_parallelized=False,\n",
    "    use_ema=True,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the Neural Stochastic Differential Equation\n",
    "\n",
    "These functions are necessary to solve the neural stochastic differential equation (SDE), as the diffusion model predicts the score function. To solve this equation the Euler-Maruyama method is employed for numerical integration while the EDM noise decay function defines the time span and noise schedule. The sampler then uses these components to generate the resulting sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "denoise_fn = trainer.inference_fn_from_state_dict(\n",
    "    trained_state,\n",
    "    denoiser=denoising_model.denoiser,\n",
    "    task='solver',\n",
    "    lead_time=False,\n",
    ")\n",
    "\n",
    "integrator = solvers.EulerMaruyama(\n",
    "    time_axis_pos=0,\n",
    "    terminal_only=True\n",
    ")\n",
    "\n",
    "tspan = dfn_lib.edm_noise_decay(\n",
    "    scheme=diffusion_scheme,\n",
    "    rho=7,\n",
    "    # Minimum of 30 steps required\n",
    "    num_steps=128,\n",
    "    end_sigma=1e-3,\n",
    "    dtype=torch.float32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "sampler = dfn_lib.SdeSampler(\n",
    "    input_shape=output_shape,\n",
    "    scheme=diffusion_scheme,\n",
    "    denoise_fn=denoise_fn,\n",
    "    tspan=tspan,\n",
    "    integrator=integrator,\n",
    "    guidance_transforms= (),\n",
    "    apply_denoise_at_end=True,\n",
    "    return_full_paths=False,\n",
    "    device=device,\n",
    "    dtype=torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and visualize resulting samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_samples, gt_samples = utils.generate_samples_2d(\n",
    "    dataloader=evaluation_dataloader, \n",
    "    time_cond=False, \n",
    "    stats_buffers=buffer_dict, \n",
    "    sampler=sampler, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "utils.plot_2d_sample(\n",
    "    gen_sample=gen_samples[0], \n",
    "    gt_sample=gt_samples[0], \n",
    "    axis=0, \n",
    "    save=False, \n",
    "    save_dir=str(os.getcwd())\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
